
module qgplsim
	include("supportFunctions.ji")
	include("localLinear_quantReg.ji")
	using Optim, LinearAlgebra, Distributions, Random, Statistics
	using Clustering

	function Base.:-(X::Matrix, xi::Vector)
		n, p = size(X)
		Xi = repeat(xi|>transpose, n, 1)
		_Xi = X - Xi
		_Xi
	end

	struct model
		X::Matrix{Float64}
		Z::Matrix{Float64}
		y::Vector{Float64}
		categ::Vector{Vector{Float64}} # cannot ensure z is Int 
		index::Dict{Vector{Float64}, Vector{Int}}
		# alpha::Vector{Float64}
		# theta::Vector{Float64}
		quantileNum::Float64
        widthExp::Float64
        intercept::Int

		function model(X, Z, y, τ = 0.5, widthExp = -0.17, intercept = 0)
			categ, index, = collection(Z)
			new(X, Z, y, categ, index, τ, widthExp, intercept)
		end


	end

	function estimator(data::model, alpha_type = 1)
		alpha_,  theta_ = x_estimator(data)
		
		if alpha_type == 0
			alpha = [-1.0, 1.0, 0.5]	
			theta = [1.0 -2.0]
		elseif alpha_type == 1
			alpha = alpha_
			theta = theta_

		end
		# alpha = alpha ./ alpha[1]
		if sum(abs.(data.Z)) < 1  # no Z 
			gamma, beta = 0, 0
		else
			gamma, beta = z_estimator(data, alpha, theta)
		end
		
		return alpha, gamma, theta, beta
	end

	function x_estimator(data::model)
		X, Z, y, τ = data.X, data.Z, data.y, data.quantileNum
		
		n, p = size(X)
		categ, index = data.categ, data.index
		ncateg = index.count

		αz = zeros(ncateg, p)
		Bz = zeros(n, p + data.intercept)
		for k in 1:ncateg
			z = categ[k]
			indz = index[z]
			nz = length(indz)
			az = zeros(nz)
			# bz = zeros(nz, p)
			yz = y[indz]
			Xz = X[indz, :]
			xzmin = minimum(Xz, dims=1)
			xzmax = maximum(Xz, dims=1)
			ht = var(Xz, dims = 1)
			# ht = xzmax - xzmin
			# ht = ones(p)
			# println(ht)
			# hh = 1
			h = 2 * ones(p) .* nz^(-1 / (p + 6))
			# println(h)

			for i in 1:p
				h = h * ht[i] 
			end
			KerVal = zeros(nz, nz)
        
			for i in 1:nz
				Xzi = Xz[i, :]
				Xi = Xz - Xzi
				# Xi = -Xi
				KerVal[:, i] = ker(Xi, h)
				if data.intercept == 1
					Xi = [ones(nz, 1) Xi]
				end
			# the accuracy of alpha plays the mvp in qgplsim, so 
			# we move the estimation of alpha to localLinear_quantReg.ji
			# for further consideration and improvement.

				modeli = localLinear_quantReg.npqr_model(yz, Xi)
				wi = localLinear_quantReg.fit(modeli, KerVal[:, i], τ, 1e-5)
				az[i] = wi[1]
				Bz[indz[i],:] = wi[2:end]

			end
		end
        	# println(Bz)
		nij = Int(n * (n - 1) / 2)
		bz = zeros(nij, p + data.intercept)
		ij = 1
		for i in 1:n - 1 # Int(n/2)
			for j in (i + 1) : n # (Int(n/2) + 1):n
				bz[ij, :] = Bz[i, :] - Bz[j, :]
				ij = ij + 1

			end
		end
		Rbz = kmeans(bz |> transpose, 2)
		# println(Rbz.centers)
       		bz1 = Rbz.centers[:, 1]
        	bz2 = Rbz.centers[:, 2]
		# 		ind1 = Rbz.assignments .== 1
		# 		ind2 = Rbz.assignments .== 2
		# 		bz1 = bz[ind1, :]
		# 		bz2 = bz[ind2, :]
		# 		n1 = sum(ind1)
		# 		n2 = nij - n1
		# 		bz1 = sum(bz1, dims = 1)
		# 		bz2 = sum(bz2, dims = 1)
		if bz1[1] > 0
			bz2 = -bz2
		else
			bz1 = -bz1
		end
		sum_Bz = bz1 + bz2
		alpha = sum_Bz / norm(sum_Bz) 
		if data.intercept == 1
			alpha_t = alpha[2:end]
			Bz_t = Bz[:, 2:end]
		else
			alpha_t = alpha
			Bz_t = Bz
		end
		C = Bz_t * alpha_t ./ norm(alpha_t)^2
		theta = sum(Bz_t - kron(C , transpose(alpha_t)) , dims = 1)
		theta = theta / n
		# alpha_global = alpha_global ./ alpha_global[1]
		alpha, theta # Cz

	end

	function z_estimator(data::model, alpha, theta)
		# alpha = alpha ./ alpha[1]
		# alpha = [1.0, 2.0] 
		# theta = [2.0 -1.0]
		n, q = size(data.Z)
		n, p = size(data.X)
		if data.intercept == 1
			X = [ones(n, 1) data.X]
		else
			X = data.X
		end
		v = X * alpha
		gamma, beta = zeros(q), zeros(q)
		vmax = maximum(v)
		vmin = minimum(v)
		y = data.y  - data.X * transpose(theta) # c1 = 0
		τ = data.quantileNum
        hp = data.widthExp
		index = deepcopy(data.index)
		categ = collect(index)
		ncateg = index.count
		vv0, vv1 = -1e5, 1e5
		v0, v1 = -1e5, 1e5
		c0, c1 = 1, -1
		# _c0, _c1 = 1, -1
		# while ncateg > 3 && c0 > c1

			C0 = zeros(ncateg)
			C1 = zeros(ncateg)
			for k in 1:ncateg
				vk = v[categ[k][2]]
				min_vk = minimum(vk)
				max_vk = maximum(vk)
				h = (length(vk))^(-0.17) * var(vk) # * (max_vk - min_vk)/2
				tail = h # (max_vk - min_vk)/10
				vmin = min_vk + tail
				vmax = max_vk - tail
				if vv0 < vmin
					vv0 = vmin
				end

				if vv1 > vmax
					vv1 = vmax
				end
			end
			v0, v1 = vv0, vv1
			nstep = 0
			step = (v1 - v0)/40

			while nstep < 5 && c0 > c1 # expand v0, v1, 20  times to find c0 < c1
				for k in 1:ncateg
					vk = v[categ[k][2]]
					yk = y[categ[k][2]]

					v_ = vk[vk .> v1]
					if v_ == []
						v_ = [v1]
					end
					_v = vk[vk .< v0]
					if _v == []
						_v = [v0]
					end
					
					v_ = minimum(v_) # vk[vk .> v1]
					_v = maximum(_v) # vk[vk .< v0]

					C0[k] = Gz(_v, vk, yk, 1, τ)[1]
					C1[k] = Gz(v_, vk, yk, 1, τ)[1]
				end
			
				c0 = maximum(C0)
				c1 = minimum(C1)

                # println("c: ", c0, ", ", c1)
                # println("v: ", v0, ", ", v1)
				if c0 > c1 
		 			v0 = v0 - step
		 			v1 = v1 + step
                    nstep = nstep + 1
		 		end
		 	end
		# 	cc = C0 + C1
		# 	kmax = argmax(cc)
		# 	kmin = argmin(cc)
		# 	if c0 > c1
		# 		if ncateg > 4
		# 			delete!(index, categ[kmin][1])
		# 			# delete!(index, categ[kmax][1])
		# 		else
		# 			delete!(index, categ[kmax][1])
		# 		end
		# 	end
		# 	ncateg = index.count

		# end # while 

		# if ncateg == 3 && c0 > c1
		# end
		# println("test")
		# 		println("c1 and c0: ", c1 ," ", c0, " ", "v1 - v0: ", v1 - v0)
		for i in 1:ncateg
			ΔdJ = zeros(ncateg - 1)
			ΔZ = transpose(hcat(keys(index)...))
			categ = collect(index)
			ΔZ = ΔZ[1:end, :] - ΔZ[i, :]
			tdz = ΔZ[i, :]
			ΔZ[i, :] = ΔZ[1, :]
			ΔZ = ΔZ[2:end, :]

			tdc = categ[i, :]
			categ[i, :] = categ[1, :]
			categ[1, :] = tdc
			# ord_gl = 5
			vz = v[categ[1][2]]
			yz = y[categ[1][2]]
			dg1(u) = Gz(u, vz, yz, 1, τ, hp)[1]

			ΔdJ = ΔdJ .- glquad(dg1, v0, v1, c0, c1)
			for k in 1:ncateg - 1
				
				vk = v[categ[k + 1][2]]
				yk = y[categ[k + 1][2]]
				dgk(u) = Gz(u, vk, yk, 1, τ)[1]
				ΔdJ[k] = ΔdJ[k] + glquad(dgk, v0, v1, c0, c1)
			end
            # println(ΔdJ)
			gammai = inv(transpose(ΔZ)*ΔZ + 0.000001 .* I(q)) * transpose(ΔZ) * ΔdJ ./(c1 - c0)
            		# println(i, ": ", gammai)
      			gamma = gamma + gammai 

			# estimation for β
			z1 = categ[1][1]
			ΔJ = zeros(ncateg - 1)
			
			vz = v[categ[1][2]]
			yz = y[categ[1][2]]
			g1(u) = Gz(u, vz, yz, 0, τ)[1]
			d1 =  sum(z1 .* gammai)
			ΔJ = ΔJ .+ glquad(g1, v0 - d1, v1 - d1, -1e5, 1e5)

			for k in 1:ncateg - 1
				zk = categ[k][1]
				vk = v[categ[k + 1][2]]
				yk = y[categ[k + 1][2]]
				gk(u) = Gz(u, vk , yk, 0, τ)[1]
				dk = sum(zk .* gammai)
				ΔJ[k] = ΔJ[k] - glquad(gk, v0 - dk, v1 - dk, -1e5, 1e5)

			end

			ΔZ = - ΔZ
			# ng = gamma ./ norm(gamma)
			# beta = ng[end:-1:1] # only for q = 2
			# beta[1] = -beta[1]
			# w = ΔJ ./ (ΔZ * beta) / (v1 - v0)
			# beta = mean(w) .* beta
			betai = inv(transpose(ΔZ)*ΔZ + 0.000001 * I(q)) * transpose(ΔZ) * ΔJ /abs(v1 - v0)
			beta = beta + betai
		end
# 		squ = (v1 - v0) * (c1 - c0)
		gamma/ncateg, beta/ncateg
	end

	function G(v, Z, y, tau = 0.5, hp = -0.17)
		# Estimate Q(y) = g(v + Zγ)
		# for v, return gz = g.(v) and dgz = ∇g.(v) 

		n, q = size(Z)
		categ, index, = collection(Z)
		ncateg = index.count
		Gz = zeros(n)
		DGz = zeros(n)
		
		for k in 1:ncateg
			z = categ[k]
			indz = index[z]
			nz = length(indz)
			dgz = zeros(nz)
			gz = zeros(nz)
			yz = y[indz]
			vz = v[indz]
			h = (nz)^(hp) * var(vz)
			KerVal = zeros(nz, nz)

			for i in 1:nz
				KerVal[:, i] = ker.((vz .- vz[i]) / h)/h
				tar_alpha(w) = sum(ρ.(yz .- w[1] - (vz .- vz[i]) * w[2], tau) .* KerVal[:, i])
				res = localLinear_quantReg.optimfunc(tar_alpha, zeros(2))
				wi = res.minimizer
            
				dgz[i] = wi[2]
				DGz[indz[i]] = wi[2]

				gz[i] = wi[1]
				Gz[indz[i]] = wi[1]
			end
		end
		Gz, DGz

	end


	function Gz(v, vz, yz, d = 1, tau = 0.5, hp = -0.17)
		# Estimate Q(y, Z = zk) = g(v + Zk γ)
		# for v, return gz = g.(v) and dgz = ∇g.(v) 

		nz = length(vz)
		nv = length(v)
		Gz = zeros(nv)
		DGz = zeros(nv)
		var_v = var(vz)
		h = (nz)^(hp) * var_v   
		
			KerVal = zeros(nz, nv)

			for i in 1:nv
				KerVal[:, i] = ker.((vz .- v[i]) / h)/h
# 			end

# 			for i in 1:nv
				tar_alpha(w) = sum(ρ.(yz .- w[1] - (vz .- v[i]) * w[2], tau) .* KerVal[:, i])
				res = localLinear_quantReg.optimfunc(tar_alpha, zeros(2), 1e-6)
				wi = res.minimizer
				DGz[i] = wi[2]
				Gz[i] = wi[1]

			end
			if d == 1
				return DGz
			else
				return Gz
			end
	end

end # module
