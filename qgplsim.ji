
module qgplsim
include("supportFunctions.ji")
include("localLinear_quantReg.ji")
using Optim, LinearAlgebra, Distributions, Random, Statistics
using Clustering

function Base.:-(X::Matrix, xi::Vector)
    n, p = size(X)
    for i in 1:n
        X[i, :] = X[i, :] - xi
    end
    X
end

struct model
	X::Matrix{Float64}
	Z::Matrix{Int}
	y::Vector{Float64}
	categ::Vector{Vector{Int}}
	index::Dict{Vector{Int}, Vector{Int}}
	# alpha::Vector{Float64}
	# theta::Vector{Float64}
	quantileNum::Float64

	function model(X, Z, y, τ = 0.5)
		categ, index, = collection(Z)
		new(X, Z, y, categ, index, τ)
	end


end

function estimator(data::model, alpha_type = 1)
	alpha_g,  dGz, = alpha_estimator(data)
    
	if alpha_type == 0
		alpha = [1.0, 0.5]	
        theta = [1.0  -2.0]
	elseif alpha_type == 1
		alpha = alpha_g
        theta = theta_estimator(alpha_g, dGz)

	end
	if sum(abs.(data.Z)) < 1  # no Z 
		gamma, beta = 0, 0
	else
		gamma, beta = z_estimator(data, alpha, theta)
	end
    
	alpha, gamma, theta, beta
end

function alpha_estimator(data::model)
	X, Z, y, τ = data.X, data.Z, data.y, data.quantileNum
	
	n, p = size(X)
	α = zeros(p)
	categ, index = data.categ, data.index
	ncateg = index.count

	sign_alpha = []
	αz = zeros(ncateg, p)
	Bz = zeros(n, p)
	for k in 1:ncateg
		z = categ[k]
		indz = index[z]
		nz = length(indz)
		az = zeros(nz)
		bz = zeros(nz, p)
		yz = y[indz]
		Xz = X[indz, :]
		# xzmin = minimum(Xz, dims=1)
		# xzmax = maximum(Xz, dims=1)
		# ht = xzmax - xzmin
		# println(ht)
		# hh = 1
		# for i in 1:p
		# 	hh = hh * ht[i]
		# end
		KerVal = zeros(nz, nz)
		h = 2 * ones(p) .* nz^(-1 / (p + 4))
		println("width in alpha :", h)
		for i in 1:nz
            Xi = Xz - Xz[i, :]
			# Xi = -Xi
            KerVal[:, i] = ker(Xi, h)

		# the accuracy of alpha plays the mvp in qgplsim, so 
		# we move the estimation of alpha to localLinear_quantReg.ji
		# for further consideration and improvement.

			modeli = localLinear_quantReg.npqr_model(yz, Xi)
			wi = localLinear_quantReg.fit(modeli, KerVal, τ)
			az[i] = wi[1]
			Bz[indz[i],:] = wi[2:end]

		end
	end

    nij = Int(n * (n - 1) / 2)
    bz = zeros(nij, p)
    ij = 1
	for i in 1:n - 1 # Int(n/2)
		for j in (i + 1) : n # (Int(n/2) + 1):n
			bz[ij, :] = Bz[i, :] - Bz[j, :]
            ij = ij + 1

        end
	end
    
    Rbz = kmeans(bz |> transpose, 2)
    ind1 = Rbz.assignments .== 1
    ind2 = Rbz.assignments .== 2
    bz1 = bz[ind1, :]
    bz2 = bz[ind2, :]
    n1 = sum(ind1)
    n2 = nij - n1
    bz1 = abs.(sum(bz1, dims = 1))
    bz2 = abs.(sum(bz2, dims = 1))
    sum_Bz = transpose((bz1 + bz2) / nij)
	alpha_global = sum_Bz / norm(sum_Bz) 
	# alpha_global = 1 / alpha_global[1] .* alpha_global
	alpha_global, Bz , αz # Cz

end

function theta_estimator(alpha, Bz)
	n, p = size(Bz)
	C = Bz * alpha ./ norm(alpha)^2
	theta = sum(Bz - kron(C , transpose(alpha)) , dims = 1)
	theta / n
end

function dgz(v, vz, yz, d, τ)
	nz = length(vz)
	vmin = minimum(vz)
	vmax = maximum(vz)
	hv = vmax - vmin
	h = nz^(-0.4) * hv / 2
	# println(h)
	KerVal = ker(vz .- v, h)
	loss_dgz(w) = sum(ρ.(yz .- w[1] - (vz .- v) * w[2], τ) .* KerVal)
	res = localLinear_quantReg.optimfunc(loss_dgz, zeros(2))
	wi = res.minimizer
	dgz = wi[2]
	gz= wi[1]
	if d == 0
		return gz
	else
		return dgz
	end
end

function z_estimator(data::model, alpha, theta)
	# alpha = alpha ./ alpha[1]
	v = data.X * alpha
	n, q = size(data.Z)
	vmax = maximum(v)
	vmin = minimum(v)
	hv = vmax - vmin
    u = data.X * transpose(theta)
	y = data.y  - u
	τ = data.quantileNum
	index = deepcopy(data.index)
	categ = collect(index)
	ncateg = index.count
	h = (n/ncateg)^(-0.4) * hv / 2
	vv0, vv1 = -1e5, 1e5
	v0, v1 = -1e5, 1e5
	c0, c1 = 1, -1
	_c0, _c1 = 1, -1
	while ncateg > 3 && c0 > c1

		dg(vv, k) = dgz(vv, v[categ[k][2]], y[categ[k][2]], 1,  τ)
		C0 = zeros(ncateg)
		C1 = zeros(ncateg)
		for k in 1:ncateg
			vk = v[categ[k][2]]
			vmin = minimum(vk)
			vmax = maximum(vk)
			if vv0 < vmin
				vv0 = vmin
			end

			if vv1 > vmax
				vv1 = vmax
			end
		end
		v0 = vv0 + 2*h # 0.3 = h 
		v1 = vv1 - 2*h
		# println("first v: ", v0, " ", v1)
		nh = 0
		while nh < 30 && c0 > c1 # expand v0, v1, 30  times to find c0 < c1
			for k in 1:ncateg
				# vk = v[index[categ[k]]]
				v_ = v1 # vk[vk .> v1]
				_v = v0 # vk[vk .< v0]
				if _v == []
					C0[k] = -1e5
				else
					C0[k] = (dg.(_v, k))
				end
				
				if v_ == []
					C1[k] = 1e5
				else
					C1[k] = (dg.(v_, k))
				end
			end
		

			c0 = maximum(C0)
			c1 = minimum(C1)
			
			# _c0 = minimum(C0) # not for the time being
			# _c1 = maximum(C1)

			if c0 > c1 

				# if _c0 <= _c1
					v0 = v0 - h/10
					v1 = v1 + h/10
					nh = nh + 1
				# else # _c0 > _c1
					# c0 = _c1
					# c1 = _c0
					# v1, v0 = v0, v1
				# end
			
			end
		end
		# println("v0: ", v0, " v1: ", v1)
		# println("c0: ", c0, " c1: ", c1)
		cc = C0 + C1
		kmax = argmax(cc)
		kmin = argmin(cc)
		if c0 > c1
			if ncateg > 4
				delete!(index, categ[kmin][1])
				# delete!(index, categ[kmax][1])
			else
				delete!(index, categ[kmax][1])
			end
		end
		ncateg = index.count
		categ = collect(index)

	end # while 

	if ncateg == 3 && c0 > c1
		
	end
	# println("final c: ", c0, " ", c1)
	# println("final v: ", v0, " ", v1)
	# println(ncateg)
	# println(v0, " ", v1, "\n",  c0, " ", c1)
	ΔdJ = zeros(ncateg - 1)
	ΔZ = transpose(hcat(keys(index)...))
	ΔZ = ΔZ[2:end, :] - ΔZ[1, :]
	dgv(vv, k) = dgz(vv, v[categ[k][2]], y[categ[k][2]], 1, τ)
	dg1(u) = dgv(u, 1)

    	ord_gl = 11
	ΔdJ = ΔdJ .- glquad(dg1, v0, v1, c0, c1, ord_gl)
	for k in 1:ncateg - 1
		dgk(u) = dgv(u, k + 1)
		ΔdJ[k] += glquad(dgk, v0, v1, c0, c1, ord_gl)
	end
	gamma = inv(transpose(ΔZ)*ΔZ + 0.00001 * I(q)) * transpose(ΔZ) * ΔdJ /(c1 - c0)
	# println("gamma = ", gamma)
	# gamma = [1.0, 0.5]
	v = v + data.Z * gamma
	# estimation for β
	gv(vv, k) =dgz(vv, v[categ[k][2]], y[categ[k][2]], 0, τ)
	# nij = Int((ncateg - 1)*ncateg/2)
	g1(u) = gv(u, 1)
	
	ΔJ = zeros(ncateg - 1)
	ΔJ = ΔJ .- glquad(g1, v0, v1, -1e5, 1e5, ord_gl)

	for k in 1:ncateg - 1
		gk(u) = gv(u, k + 1)
		ΔJ[k] += glquad(gk, v0, v1, -1e5, 1e5, ord_gl)

	end

	beta = inv(transpose(ΔZ)*ΔZ + 0.00001 * I(q)) * transpose(ΔZ) * ΔJ /abs(v1 - v0)
	gamma, beta
end

function Gz_DGz(v, Z, y, tau = 0.5, hp = -0.4)
	# Estimate Q(y) = g(v + Zγ)
	# for v, return gz = g.(v) and dgz = ∇g.(v) 

	n, q = size(Z)
	categ, index, = collection(Z)
	ncateg = index.count
	Gz = zeros(n)
	DGz = zeros(n)
	for k in 1:ncateg
		z = categ[k]
		indz = index[z]
		nz = length(indz)
		dgz = zeros(nz)
		gz = zeros(nz)
		yz = y[indz]
		vz = v[indz]
        vmin = minimum(vz)
        vmax = maximum(vz)
        hv = vmax - vmin
		KerVal = zeros(nz, nz)
		h = 2*nz^(hp) * hv / 2
		for i in 1:nz
			KerVal[:, i] = ker.((vz .- vz[i]) / h)/h
		end

		for i in 1:nz

			tar_alpha(w) = sum(ρ.(yz .- w[1] - (vz .- vz[i]) * w[2], tau) .* KerVal[:, i])
			res = localLinear_quantReg.optimfunc(tar_alpha, zeros(2))
			wi = res.minimizer
			dgz[i] = wi[2]
			DGz[indz[i]] = wi[2]

			gz[i] = wi[1]
			Gz[indz[i]] = wi[1]

		end
	end
	Gz, DGz

end


end # module