
module qgplsim
include("supportFunctions.ji")
include("localLinear_quantReg.ji")
using Optim, LinearAlgebra, Distributions, Random, Statistics
using Clustering

function Base.:-(X::Matrix, xi::Vector)
    n, p = size(X)
    for i in 1:n
        X[i, :] = X[i, :] - xi
    end
    X
end

struct model
	X::Matrix{Float64}
	Z::Matrix{Int}
	y::Vector{Float64}
	categ::Vector{Vector{Int}}
	index::Dict{Vector{Int}, Vector{Int}}
	# alpha::Vector{Float64}
	# theta::Vector{Float64}
	quantileNum::Float64

	function model(X, Z, y, τ = 0.5)
		categ, index, = collection(Z)
		new(X, Z, y, categ, index, τ)
	end


end

function estimator(data::model, alpha_type = 1)
	alpha_g,  dGz, = alpha_estimator(data)
    
	if alpha_type == 0
		alpha = [1.0, 0.5]	
        theta = [1.0  -2.0]
	elseif alpha_type == 1
		alpha = alpha_g
        theta = theta_estimator(alpha_g, dGz)

	end
	if sum(abs.(data.Z)) < 1  # no Z 
		gamma = 0
	else
		gamma = gamma_estimator(data, alpha, theta)
	end
    
    beta = beta_estimator(data, alpha, theta, gamma)
	alpha, gamma, theta, beta
end

function alpha_estimator(data::model)
	X, Z, y, τ = data.X, data.Z, data.y, data.quantileNum
	
	n, p = size(X)
	α = zeros(p)
	categ, index = data.categ, data.index
	ncateg = index.count

	sign_alpha = []
	αz = zeros(ncateg, p)
	Bz = zeros(n, p)
	for k in 1:ncateg
		z = categ[k]
		indz = index[z]
		nz = length(indz)
		az = zeros(nz)
		bz = zeros(nz, p)
		yz = y[indz]
		Xz = X[indz, :]
		# xzmin = minimum(Xz, dims=1)
		# xzmax = maximum(Xz, dims=1)
		# ht = xzmax - xzmin
		# println(ht)
		# hh = 1
		# for i in 1:p
		# 	hh = hh * ht[i]
		# end
		KerVal = zeros(nz, nz)
		h = 2 * ones(p) .* nz^(-2 / (p + 4))
		
		for i in 1:nz
            Xi = Xz - Xz[i, :]
			# Xi = -Xi
            KerVal[:, i] = ker(Xi, h)

		# the accuracy of alpha plays the mvp in qgplsim, so 
		# we move the estimation of alpha to localLinear_quantReg.ji
		# for further consideration and improvement.

			modeli = localLinear_quantReg.npqr_model(yz, Xi)
			wi = localLinear_quantReg.fit(modeli, KerVal, τ)
			az[i] = wi[1]
			Bz[indz[i],:] = wi[2:end]

		end
	end

    nij = Int(n * (n - 1) / 2)
    bz = zeros(nij, p)
    ij = 1
	for i in 1:n - 1 # Int(n/2)
		for j in (i + 1) : n # (Int(n/2) + 1):n
			bz[ij, :] = Bz[i, :] - Bz[j, :]
            ij = ij + 1

        end
	end
    
    Rbz = kmeans(bz |> transpose, 2)
    ind1 = Rbz.assignments .== 1
    ind2 = Rbz.assignments .== 2
    bz1 = bz[ind1, :]
    bz2 = bz[ind2, :]
    n1 = sum(ind1)
    n2 = nij - n1
    bz1 = abs.(sum(bz1, dims = 1))
    bz2 = abs.(sum(bz2, dims = 1))
    sum_Bz = transpose((bz1 + bz2) / nij)
	alpha_global = sum_Bz / norm(sum_Bz) 
	# alpha_global = 1 / alpha_global[1] .* alpha_global
	alpha_global, Bz , αz # Cz

end

function theta_estimator(alpha, Bz)
	n, p = size(Bz)
	C = Bz * alpha ./ norm(alpha)^2
	theta = sum(Bz - kron(C , transpose(alpha)) , dims = 1)
	theta / n
end

function dgz(v, uz, vz, yz, τ)
	nz = length(vz)
	vmin = minimum(vz)
	vmax = maximum(vz)
	hv = vmax - vmin
	h = nz^(-0.4) * hv / 2
	# println(h)
	KerVal = ker(vz .- v, h)
	loss_dgz(w) = sum(ρ.(yz .- w[1] - (vz .- v) * w[2], τ) .* KerVal)
	res = localLinear_quantReg.optimfunc(loss_dgz, zeros(2))
	wi = res.minimizer
	dgz = wi[2]

	# gz= wi[1]
	dgz
end

function gamma_estimator(data::model, alpha, theta)
	v = data.X * alpha
    u = data.X * transpose(theta)
	y = data.y  - data.X * transpose(theta)
	τ = data.quantileNum
	categ, index = data.categ, data.index
	ncateg = index.count
	ΔJ = zeros(ncateg - 1)
	ΔZ = transpose(hcat(categ...))
	ΔZ = ΔZ[2:end, :] - ΔZ[1, :]
	v0, v1 = -1e5, 1e5
	C0 = zeros(ncateg)
	C1 = zeros(ncateg)
	dg(vv, k) = dgz(vv, u[index[categ[k]]], v[index[categ[k]]], y[index[categ[k]]], τ)

	for k in 1:ncateg
		vk = v[index[categ[k]]]
		vmin = minimum(vk)
		vmax = maximum(vk)
		if v0 < vmin
			v0 = vmin
		end

		if v1 > vmax
			v1 = vmax
		end
	end

	for k in 1:ncateg
		vk = v[index[categ[k]]]
		v_ = v1 # vk[vk .> v1]
		_v = v0 # vk[vk .< v0]
		if _v == []
			C0[k] = -1e5
		else
			C0[k] = (dg.(_v, k))
		end
		
		if v_ == []
			C1[k] = 1e5
		else
			C1[k] = (dg.(v_, k))
		end
	end
	c0 = maximum(C0)
	c1 = minimum(C1)
	# println("v0: ", v0, " v1: ", v1)
	# println("c0: ", c0, " c1: ", c1)

	dg1(u) = dg(u, 1)
    	ord_gl = 5
	ΔJ = ΔJ .- glquad(dg1, v0, v1, c0, c1, ord_gl)
	for k in 1:ncateg - 1
		dgk(u) = dg(u, k + 1)
		ΔJ[k] += glquad(dgk, v0, v1, c0, c1, ord_gl)
	end
	gamma = inv(transpose(ΔZ)*ΔZ) * transpose(ΔZ) * ΔJ /(c1 - c0)
	gamma

end

function Gz_DGz(v, Z, y, hp = -0.2)

	n, q = size(Z)
	categ, index, = collection(Z)
	ncateg = index.count
	Gz = zeros(n)
	DGz = zeros(n)
	for k in 1:ncateg
		z = categ[k]
		indz = index[z]
		nz = length(indz)
		dgz = zeros(nz)
		gz = zeros(nz)
		yz = y[indz]
		vz = v[indz]
		KerVal = zeros(nz, nz)
		h = 2*nz^(hp)
		for i in 1:nz
			KerVal[:, i] = ker.((vz .- vz[i]) / h)/h
		end

		for i in 1:nz

			tar_alpha(w) = sum(ρ.(yz .- w[1] - (vz .- vz[i]) * w[2]) .* KerVal[:, i])
			res = localLinear_quantReg.optimfunc(tar_alpha, zeros(2))
			wi = res.minimizer
			dgz[i] = wi[2]
			DGz[indz[i]] = wi[2]

			gz[i] = wi[1]
			Gz[indz[i]] = wi[1]

		end
	end
	Gz, DGz

end

function beta_estimator(data::model, alpha, theta, gamma)
    v = data.X * alpha # g(Xα + Zγ) => univar func. g(v + zγ) after α was determined and  Z = z
    u = data.X * transpose(theta)
    categ, index = data.categ, data.index
    a, b = qgplsim.Gz_DGz(v, data.Z, data.y) # g: g(v), dg: g'(v)
    n = length(data.y)
    
    nij = n*(n - 1)/2
    nij = Int(nij)
    am = zeros(nij)
    um = zeros(nij)
    # vm = zeros(nij)
    bm = zeros(nij)
    zm = zeros(nij, 2)
    c = 1
    for i in 1:n - 1
        for j in  (i + 1):n
            zm[c, :] = data.Z[j, :] - data.Z[i, :]
			um[c] = u[j] - u[i]
            am[c] = a[j] - a[i]
            bm[c] = b[i].*(transpose(zm[c, :]) * gamma + v[j] - v[i]) 
            c += 1
        end
    end
    beta = inv(transpose(zm) * (zm)) * transpose(zm) * (am - um - bm)
    beta
end

end # module