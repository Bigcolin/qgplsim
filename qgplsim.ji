
module qgplsim
include("supportFunctions.ji")
include("localLinear_quantReg.ji")
using Optim, LinearAlgebra, Distributions, Random, Statistics
function Base.:-(X::Matrix, xi::Vector)
    n, p = size(X)
    for i in 1:n
        X[i, :] = X[i, :] - xi
    end
    X
end

struct model
	X::Matrix{Float64}
	Z::Matrix{Int}
	y::Vector{Float64}
	categ::Vector{Vector{Int}}
	index::Dict{Vector{Int}, Vector{Int}}
	# alpha::Vector{Float64}
	# theta::Vector{Float64}
	quantileNum::Float64

	function model(X, Z, y, τ = 0.5)
		categ, index, = collection(Z)
		new(X, Z, y, categ, index, τ)
	end


end

function estimator(data::model, alpha_type = 1)
	alpha_g, alpha_l, dGz, = alpha_estimator(data)
	
	if alpha_type == 0
		alpha = [1.0, -1.0]	
	elseif alpha_type == 1
		alpha = alpha_g
	elseif alpha_type == 2 
		alpha = alpha_l
	end
	theta = theta_estimator(alpha, dGz)
	if sum(abs.(data.Z)) < 1  # no Z 
		gamma = 0
	else
		gamma = gamma_estimator(data, alpha, theta)
	end
    
    beta = beta_estimator(data, alpha, theta, gamma)
	alpha, theta, gamma, beta
end

function alpha_estimator(data::model)
	X, Z, y, τ = data.X, data.Z, data.y, data.quantileNum
	
	n, p = size(X)
	α = zeros(p)
	categ, index = data.categ, data.index
	ncateg = index.count

	sign_alpha = []
	αz = zeros(ncateg, p)
	Bz = zeros(n, p)
	for k in 1:ncateg
		z = categ[k]
		indz = index[z]
		nz = length(indz)
		az = zeros(nz)
		bz = zeros(nz, p)
		yz = y[indz]
		Xz = X[indz, :]
		KerVal = zeros(nz, nz)
		h = 2 * ones(p) .* nz^(-2 / (p + 4))
		for i in 1:nz
            		Xi = Xz - Xz[i, :]
			KerVal[:, i] = ker(Xi, h)

		# the accuracy of alpha plays the mvp in qgplsim, so 
		# we move the estimation of alpha to localLinear_quantReg.ji
		# for further consideration and improvement.

			modeli = localLinear_quantReg.npqr_model(yz, Xi)
			wi = localLinear_quantReg.fit(modeli, KerVal, τ)
			az[i] = wi[1]
			bz[i,:] = wi[2:end]
			Bz[indz[i],:] = wi[2:end]

		end

		sum_bz = zeros(p)
		for i in 1:nz - 1
			for j in i + 1:nz
				sum_bz += (bz[i, :] - bz[j, :])
			end
		end
		αz[k,:] = sum_bz / norm(sum_bz) # norm(α)

		push!(sign_alpha, sign.(αz[k, :])) 
		α += abs.(αz[k, :]) * nz / n

	end
	c, d, sign_alpha = collection(sign_alpha)

	alpha_local = α.* sign_alpha[1]
	sum_Bz = zeros(p)
	for i in 1:n - 1
		for j in i + 1:n
			sum_Bz += (Bz[i, :] - Bz[j, :])
		end
	end
	alpha_global = sum_Bz / norm(sum_Bz)
	alpha_global = sign.(alpha_global)[1] .* alpha_global
	alpha_global, alpha_local,  Bz , αz # Cz

end

function theta_estimator(alpha, Bz)
	n, p = size(Bz)
	C = Bz * alpha ./ norm(alpha)^2
	theta = sum(Bz - kron(C , transpose(alpha)) , dims = 1)
	theta / n
end

function dgz(v, vz, yz, τ)
	nz = length(vz)
	h = 2*nz^(-0.4)
	KerVal = ker(v .- vz, h)
	loss_dgz(w) = sum(ρ.(yz .- w[1] - (v .- vz) * w[2], τ) .* KerVal)
	res = localLinear_quantReg.optimfunc(loss_dgz, zeros(2))
	wi = res.minimizer
	dgz = wi[2]

	# gz= wi[1]
	dgz
end

function gamma_estimator(data::model, alpha, theta)
	v = data.X * alpha
	y = data.y - data.X * transpose(theta)
	τ = data.quantileNum
	categ, index = data.categ, data.index
	ncateg = index.count
	ΔJ = zeros(ncateg - 1)
	ΔZ = transpose(hcat(categ...))
	ΔZ = ΔZ[2:end, :] - ΔZ[1, :]
	v0, v1 = -1e5, 1e5
	C0 = zeros(ncateg)
	C1 = zeros(ncateg)
	dg(u, k) = dgz(u, v[index[categ[k]]], y[index[categ[k]]], τ)

	for k in 1:ncateg
		vk = v[index[categ[k]]]
		vmin = minimum(vk)
		vmax = maximum(vk)
		if v0 < vmin
			v0 = vmin
		end

		if v1 > vmax
			v1 = vmax
		end
	end

	for k in 1:ncateg
		C0[k] = dg(v0, k)
		C1[k] = dg(v1, k)
	end
	c0 = minimum(C0)
	c1 = maximum(C1)

	dg1(u) = dg(u, 1)
	ΔJ = ΔJ .- glquad(dg1, v0, v1, c0, c1)
	for k in 1:ncateg - 1
		dgk(u) = dg(u, k + 1)
		ΔJ[k] += glquad(dgk, v0, v1, c0, c1)
	end
	gamma = inv(transpose(ΔZ)*ΔZ) * transpose(ΔZ) * ΔJ /(c1 - c0)
	if sum(gamma) < 0
		gamma = - gamma
	end
	2 * gamma

end

function Gz_DGz(v, Z, y)

	n, q = size(Z)
	categ, index, = collection(Z)
	ncateg = index.count
	Gz = zeros(n)
	DGz = zeros(n)
	for k in 1:ncateg
		z = categ[k]
		indz = index[z]
		nz = length(indz)
		dgz = zeros(nz)
		gz = zeros(nz)
		yz = y[indz]
		vz = v[indz]
		KerVal = zeros(nz, nz)
		h = 2*nz^(-0.4)
		for i in 1:nz
			KerVal[:, i] = ker.((vz .- vz[i]) / h)/h
		end

		for i in 1:nz

			tar_alpha(w) = sum(ρ.(yz .- w[1] - (vz .- vz[i]) * w[2]) .* KerVal[:, i])
			res = localLinear_quantReg.optimfunc(tar_alpha, zeros(2))
			wi = res.minimizer
			dgz[i] = wi[2]
			DGz[indz[i]] = wi[2]

			gz[i] = wi[1]
			Gz[indz[i]] = wi[1]

		end
	end
	Gz, DGz

end

function beta_estimator(data::model, alpha, theta, gamma)
    v = data.X * alpha # g(Xα + Zγ) => univar func. g(v + zγ) after α was determined and  Z = z
    u = data.X * transpose(theta)
    categ, index = data.categ, data.index
    a, b = qgplsim.Gz_DGz(v, data.Z, data.y) # g: g(v), dg: g'(v)
    n = length(data.y)
    
    nij = n*(n - 1)/2
    nij = Int(nij)
    am = zeros(nij)
    um = zeros(nij)
    # vm = zeros(nij)
    bm = zeros(nij)
    zm = zeros(nij, 2)
    c = 1
    for i in 1:n - 1
        for j in  (i + 1):n
            zm[c, :] = data.Z[j, :] - data.Z[i, :]
	    um[c] = u[j] - u[i]
            am[c] = a[j] - a[i]
            bm[c] = b[i].*(transpose(zm[c, :]) * gamma + v[j] - v[i]) 
            c += 1
        end
    end
    beta = inv(transpose(zm) * (zm)) * transpose(zm) * (am - um - bm)
    beta
end

end # module